{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.3.8 Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loding the data\n",
    "\n",
    "The data is composed of 4436 documents that are classified in two different classes '0' and '1'. Class 0 has 2840 entries and class 1 has 1596.\n",
    "\n",
    "first column is the class and second column is the text of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv', header=None)\n",
    "print('size of class 0:', sum(data[0] == 0))\n",
    "print('size of class 1:', sum(data[0] == 1))\n",
    "\n",
    "#show the entry 12\n",
    "print(data[1].iloc[12]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "We need to find a way to represent individual observations of texts as a row, and encode a static number of features, represented as columns, across all of these observations. As such, feature extraction becomes the most important aspect of text preprocessing.\n",
    "\n",
    "If you have a look at the data, you will realize that the data is kind of clean. We still want to get rid of stop words, which are usually defined as very common words in a given language.\n",
    "\n",
    "Although you can find ways to implement this from scratch, I suggest you utilize the Natural Language Toolkit (NLTK) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords # we need nltk corpus to be installed\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "#nltk.download()  #uncomment this if nltk is not installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next cell we are to remove all stop words found in each document in data. We need also to split the whole text into words. \n",
    "\n",
    "``\n",
    "Example:\n",
    "Original text:    \"expects earnings in to increase at least to told\"\n",
    "After processing: 'expects', 'earnings', 'increase', 'least', 'told'\n",
    "``\n",
    "\n",
    "<span style=\"color:red\">**Note:**</span> Make sure you remove the empty string (' ') at the end of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word tokenization__: It is simply the process of separating a single string object, usually a body of text of varying length, into individual tokens that represent words that we would like to evaluate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: tokens\n",
    "\n",
    "Now the text is seen as tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data[1].iloc[12]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "['standard', 'trustco', 'sees', 'better', 'year', 'standard', 'trustco', 'said', 'expects', 'earnings', 'increase', 'least', 'pct', 'dlrs', 'dlrs', 'per', 'share', 'recorded', 'stable', 'interest', 'rates', 'growing', 'economy', 'expected', 'provide', 'favorable', 'conditions', 'growth', 'president', 'brian', 'malley', 'told', 'shareholders', 'annual', 'meeting', 'standard', 'trustco', 'previously', 'reported', 'assets', 'billion', 'dlrs', 'billion', 'dlrs', 'return', 'common', 'shareholders', 'equity', 'pct', 'last', 'year', 'pct', 'reuter']\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary of words\n",
    "\n",
    "Now we are going to find the words in all the documents and count the number of times a word is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the frequency of the different words in the list tmp2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ?\n",
    "print(len(words), 'total of different words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "15787 total of different words\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the words\n",
    "\n",
    "Now, you need to sort the words and get the **N** most influential words. The influential words are the ones that are seen more frecuently in all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "Influential words 1000\n",
    "[('vs', 9158), ('mln', 9034), ('said', 7449), ('dlrs', 5929), ('cts', 5510)]\n",
    "['vs', 'mln', 'said', 'dlrs', 'cts', 'net', 'reuter', 'loss', 'year', 'company']\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the features\n",
    "Now, you need to count the number of times each word in influential appears in every document. So, you will add a column for each word in influential list into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column for each influential word and compute the number it appears for each document\n",
    "\n",
    "#??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>  Just the first row:\n",
    "\n",
    "``\n",
    "___0                                                  1  vs  mln  said  dlrs\n",
    "0  0  [champion, products, ch, approves, stock, spli...   0    2     2     0   \n",
    "1  1  [computer, terminal, systems, cpml, completes,...   0    1     7     4   \n",
    "2  0  [cobanco, inc, cbco, year, net, shr, cts, vs, ...   5    6     0     2   \n",
    "3  0  [international, inc, nd, qtr, jan, oper, shr, ...   9    8     0     0   \n",
    "4  0  [brown, forman, inc, bfd, th, qtr, net, shr, o...   6    7     0     2\n",
    ":\n",
    ".\n",
    "[5 rows x 1002 columns]\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get numpy matrix\n",
    "\n",
    "Let's shuffle the data, add a column of 1's called '`const`' and split the data to have train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "data = data1.sample(frac=1).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function `train_test_split` to split the data, use 80% for training and 20% for testing.\n",
    "\n",
    "recall that features for our X are **`'const' + influential`**, target data **`y`** is the column `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data1['const'] = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1[ ['const'] + influential], data1[0], test_size=0.2)\n",
    "print('Training data: ', X_train.shape, y_train.shape)\n",
    "print('Testing data: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get numpy data from DataFrame\n",
    "X = np.array(X_train)\n",
    "y = np.array(y_train).reshape(-1,1)\n",
    "X_t = np.array(X_test)\n",
    "y_t = np.array(y_test).reshape(-1,1)\n",
    "print('X', X.shape, 'y', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "X (3548, 1001) y (3548, 1)\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Implement next functions to finally implement your gradient descent, you already know this functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute logistic function\n",
    "def h(x, w):\n",
    "    #?\n",
    "    return s     \n",
    "\n",
    "# Compute the cost\n",
    "def cost(x, y, w):\n",
    "   #?\n",
    "    #print(\"costo =\", c.shape)\n",
    "    return c\n",
    "\n",
    "# Compute the gradient for all w's, do not use regularization\n",
    "def grad(x, y, w):\n",
    "    #?\n",
    "    return g\n",
    "    \n",
    "# gradient descent\n",
    "def gd(x, y, w, alpha = 0.001, its = 1000):\n",
    "    J = {}\n",
    "    costo = 0\n",
    "\n",
    "    for i in range(its):\n",
    "        # Gradient\n",
    "        gradiente = ?\n",
    "            \n",
    "        #Cost function\n",
    "        costo = ?\n",
    "      \n",
    "        #Updating w\n",
    "        w =?\n",
    "                \n",
    "        #Storing cost\n",
    "        J[i] = costo    \n",
    "    return w, J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It will take **9.5 hours** to train... just kidding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "alpha = 0.01\n",
    "its = 8000 #once it is working you can test increasing the number of iterations\n",
    "w = np.zeros((1, n))\n",
    "print('cost from train:', cost(X, y, w))\n",
    "time_start = time.clock()\n",
    "w, J = gd(X, y, w, alpha, its)\n",
    "time_elapsed = (time.clock() - time_start)\n",
    "print('cost from train after training:', cost(X, y, w))\n",
    "print('GD took', time_elapsed/60, 'minutes for', its, 'iterations')\n",
    "w = w.reshape(-1)\n",
    "print(w)\n",
    "\n",
    "#Plot the cost over the iterations\n",
    "plt.plot(J.keys(), J.values())\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "cost from train: 0.69314718056\n",
    "cost from train after training: 0.0609401997241\n",
    "GD took 9.595006166666668 minutes for 8000 iterations            <<< Do not expect it to be same\n",
    "[ 0.45577217 -0.99538671 -0.24285189 ..., -0.03876273  0.02442601\n",
    " -0.03338738]\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Fraction of correctly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns fraction and correctly classified\n",
    "#fraction  = (true positives + true negatives) / total_samples\n",
    "def accuracy(x, y, w):\n",
    "    #?\n",
    "    return acc, tptn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(y)\n",
    "score, tptn = accuracy(X, y, w)\n",
    "print('Accuracy in train =', score)\n",
    "print(tptn, 'out of', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "Accuracy in train = 0.98900789177\n",
    "3509 out of 3548\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracy and cost in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = np.array(y_test).reshape(-1,1)\n",
    "m = len(yt)\n",
    "Xt = np.array(X_test)\n",
    "score, tptn = accuracy(Xt, yt, w)\n",
    "print('Accuracy in train =', score)\n",
    "print(tptn, 'out of', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "Accuracy in train = 0.986486486486\n",
    "876 out of 888\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity and specificity\n",
    "\n",
    "To compute sensitivity and specificity we need to extract four summary values from our classification results. These are the numbers of:\n",
    "\n",
    "1. True positives (TP) – the number of objects with class = 1 that are classified as 1\n",
    "2. True negatives (TN) – the number of objects with class = 0 that are classified as 0\n",
    "- False positives (FP) – the number of objects with class = 0 that are classified as 1\n",
    "- False negatives (FN) – the number of objects with class = 1 that are classified as 0\n",
    "\n",
    "we compute sensitivity (recall) as: $s_e = \\frac{TP}{TP + FN}$\n",
    "\n",
    "and specificity as: $s_p = \\frac{TN}{TN + FP}$\n",
    "\n",
    "Both values lie between 0 and 1.\n",
    "\n",
    "consider using for computing `values` function:\n",
    "\n",
    "```python\n",
    "sum(y_true & pred_true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p is the probability\n",
    "# threshold, class is 1 if probability >= threshold\n",
    "def predict(p, threshold):\n",
    "    y_hat = list(map(lambda x: 1 if x >= threshold else 0, p))\n",
    "    return y_hat\n",
    "\n",
    "# to compute sensitivity and specificity we need to extract the four values\n",
    "# returns a dictionary V={'TP':val, 'TN':val,'FN': val, 'FP':val}\n",
    "def values(y, pred):\n",
    "    tn = tp = fp = fn = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == pred[i]:\n",
    "            if y[i] == 0:\n",
    "                tn += 1\n",
    "            else:\n",
    "                tp += 1\n",
    "        else:\n",
    "            if y[i] == 0:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        \n",
    "    V = {'TP':tp, 'TN':tn, 'FN':fn, 'FP':fp}\n",
    "    return V\n",
    "\n",
    "def sensitivity(V):\n",
    "    sen = V['TP'] / (V['TP'] + V['FN'])\n",
    "    return sen\n",
    "\n",
    "def specificity(V):\n",
    "    spec = V['TN'] / (V['TN'] + V['FP'])\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: Test tour code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values = np.array([0, 1, 1, 1, 0, 1, 0, 1, 1, 0])\n",
    "p = np.array([0.2, .9, .8, 0.49, .5, .48, .02, .99, .8, .52])\n",
    "thr = 0.5\n",
    "print('y    ', y_values)\n",
    "pred = predict(p, thr)\n",
    "print('pred ', pred)\n",
    "V = values(y_values, pred)\n",
    "print(V)\n",
    "\n",
    "print('Sensitivity: ', sensitivity(V))\n",
    "print('Specificity: ', specificity(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "y     [0 1 1 1 0 1 0 1 1 0]\n",
    "pred  [0 1 1 0 1 0 0 1 1 1]\n",
    "{'TP': 4, 'TN': 2, 'FN': 2, 'FP': 2}\n",
    "Sensitivity:  0.666666666667\n",
    "Specificity:  0.5\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "\n",
    "The receiver operating characteristic (ROC) curve lets us examine how the performance varies as we change this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute sensitivity and specificity for each threshold in [0, 1]\n",
    "# return Se, Sp\n",
    "def ROC(y, p):\n",
    "    thrs = np.arange(0, 1, 0.05)\n",
    "    Se = np.zeros((len(thrs)))\n",
    "    Sp = np.zeros((len(thrs)))\n",
    "    for j in range(len(thrs)):\n",
    "        pred = predict(p,thrs[j])\n",
    "        V = values(y, pred)\n",
    "        Se[j] = sensitivity(V)\n",
    "        Sp[j] = specificity(V)\n",
    "    return Se, Sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Roc function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict(h(X, w), 0.5)\n",
    "print('Four values for training:', values(y.squeeze(), pred))\n",
    "\n",
    "Se, Sp = ROC(y, h(X, w))\n",
    "plt.plot(1 - Sp, Se, '-');\n",
    "\n",
    "pred1 = predict(h(X_t, w), 0.5)\n",
    "print('Four values for testing:', values(y_t.squeeze(), pred1))\n",
    "\n",
    "Se1, Sp1 = ROC(y_t, h(X_t, w))\n",
    "plt.plot(1 - Sp1, Se1, '-');\n",
    "\n",
    "plt.xlabel('1 - Specificity')\n",
    "plt.ylabel('Sensitivity');\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.title('Receiver Operating Characteristic');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "Four values for training: {'TP': 1237, 'TN': 2272, 'FN': 11, 'FP': 28}\n",
    "Four values for testing: {'TP': 345, 'TN': 531, 'FN': 3, 'FP': 9}\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Under the ROC curve\n",
    "\n",
    "The curve will always start at $S_e = 0, 1 − S_p = 0$, corresponding to a threshold that never classifies anything as belonging to class 1, and finish at $S_e = 1, 1 − S_p = 1$, corresponding to a classifier that never classifies anything as belonging to class 0.\n",
    "\n",
    "As the classifier gets worse, the curve will get closer to a straight line from (0, 0) to (1, 1).\n",
    "\n",
    "We can now quantify the **performance** by computing the area under the ROC curve known as the __AUC__. A classifier that is able to perfectly classify the data will have an AUC of **1**, a classifier that is guessing randomly will have an AUC of **0.5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics #use this library to compute AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train AUC: ', metrics.roc_auc_score(y, h(X, w)));\n",
    "print('Test AUC: ', metrics.roc_auc_score(y_test, h(X_test, w)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Expected output:**</span>\n",
    "\n",
    "``\n",
    "Train AUC:  0.998605072464\n",
    "Test AUC:  0.995934440187\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "\n",
    "Instead of Logistic Regression implement a Neural Network to classify de documents. Obtain The Accuracy, ROC curve, and AUC. \n",
    "\n",
    "Compare the results obtained with those from Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
